# [VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](#)

Official PyTorch implementation and pretrained models of VLMo.

- Nov 2021: release preprint in [arXiv](#)


## Citation

If you find this repository useful, please consider citing our work:
```
@article{vlmo,
      title={{VLMo}: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts}, 
      author={Wenhui Wang and Hangbo Bao and Li Dong and Furu Wei},
      year={2021},
      eprint={2111.####},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```



## License
This project is licensed under the license found in the LICENSE file in the root directory of this source tree.

[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)

### Contact Information

For help or issues using VLMo models, please submit a GitHub issue.

For other communications related to UniLM AI, please contact Li Dong (`lidong1@microsoft.com`), [Furu Wei](http://gitnlp.org/) (`fuwei@microsoft.com`).
